"""
AI Service for handling multiple AI models
"""
import asyncio
from typing import Dict, Any, List, Optional, AsyncGenerator
import json
import uuid
from datetime import datetime
from abc import ABC, abstractmethod

from app.core.config import settings
from app.models.database import User

class AIModel(ABC):
    """Abstract base class for AI models"""
    
    def __init__(self, config: dict):
        self.config = config
        self.name = config["name"]
        self.max_tokens = config["max_tokens"]
        self.temperature = config["temperature"]
    
    @abstractmethod
    async def generate(self, prompt: str, **kwargs) -> str:
        """Generate response from model"""
        pass
    
    @abstractmethod
    async def stream(self, prompt: str, **kwargs) -> AsyncGenerator[str, None]:
        """Stream response from model"""
        pass

class MockModel(AIModel):
    """Mock model for development/testing"""
    
    async def generate(self, prompt: str, **kwargs) -> str:
        await asyncio.sleep(0.5)  # Simulate processing time
        
        # Detect request type and generate appropriate response
        if any(word in prompt.lower() for word in ["ÐºÐ¾Ð´", "code", "function", "Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼"]):
            return self._generate_code_response(prompt)
        elif any(word in prompt.lower() for word in ["Ð°Ð½Ð°Ð»Ð¸Ð·", "analyze", "Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²"]):
            return self._generate_analysis_response(prompt)
        else:
            return self._generate_chat_response(prompt)
    
    async def stream(self, prompt: str, **kwargs) -> AsyncGenerator[str, None]:
        response = await self.generate(prompt, **kwargs)
        words = response.split()
        
        for i, word in enumerate(words):
            yield word + (" " if i < len(words) - 1 else "")
            await asyncio.sleep(0.05)  # Simulate streaming delay
    
    def _generate_code_response(self, prompt: str) -> str:
        return f"""ÐÐ½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÑ Ð²Ð°Ñˆ Ð·Ð°Ð¿Ñ€Ð¾Ñ Ð¾ ÐºÐ¾Ð´Ðµ, Ð²Ð¾Ñ‚ Ñ€ÐµÑˆÐµÐ½Ð¸Ðµ Ð¾Ñ‚ {self.name}:

```python
# HoYo AI Generated Solution
def process_request(data):
    \"\"\"Process user request with HoYo AI\"\"\"
    result = []
    
    for item in data:
        if validate(item):
            processed = transform(item)
            result.append(processed)
    
    return {{
        "success": True,
        "data": result,
        "model": "{self.name}",
        "timestamp": datetime.now().isoformat()
    }}

def validate(item):
    \"\"\"Validate input data\"\"\"
    return item is not None and len(item) > 0

def transform(item):
    \"\"\"Transform data using HoYo algorithms\"\"\"
    return {{
        "original": item,
        "processed": True,
        "enhanced_by": "HoYo Technologies"
    }}
```

Ð­Ñ‚Ð¾Ñ‚ ÐºÐ¾Ð´ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½ Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸ Ð¸ ÑÐ»ÐµÐ´ÑƒÐµÑ‚ Ð»ÑƒÑ‡ÑˆÐ¸Ð¼ Ð¿Ñ€Ð°ÐºÑ‚Ð¸ÐºÐ°Ð¼ Python."""
    
    def _generate_chat_response(self, prompt: str) -> str:
        return f"""ÐŸÑ€Ð¸Ð²ÐµÑ‚! Ð¯ {self.name} Ð¾Ñ‚ HoYo Technologies. 

Ð¯ Ð¿Ñ€Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð» Ð²Ð°Ñˆ Ð·Ð°Ð¿Ñ€Ð¾Ñ: "{prompt[:100]}..."

Ð’Ð¾Ñ‚ Ð¼Ð¾Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚: ÑÑ‚Ð¾ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð¸Ð½Ñ‚ÐµÑ€ÐµÑÐ½Ñ‹Ð¹ Ð²Ð¾Ð¿Ñ€Ð¾Ñ! HoYo Technologies Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ Ð¿ÐµÑ€ÐµÐ´Ð¾Ð²Ñ‹Ðµ Ñ‚ÐµÑ…Ð½Ð¾Ð»Ð¾Ð³Ð¸Ð¸ 
Ð¸ÑÐºÑƒÑÑÑ‚Ð²ÐµÐ½Ð½Ð¾Ð³Ð¾ Ð¸Ð½Ñ‚ÐµÐ»Ð»ÐµÐºÑ‚Ð° Ð´Ð»Ñ Ñ€ÐµÑˆÐµÐ½Ð¸Ñ Ð¿Ð¾Ð´Ð¾Ð±Ð½Ñ‹Ñ… Ð·Ð°Ð´Ð°Ñ‡. ÐÐ°Ñˆ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ ÑÐ¾Ñ‡ÐµÑ‚Ð°ÐµÑ‚ Ð² ÑÐµÐ±Ðµ Ð¸Ð½Ð½Ð¾Ð²Ð°Ñ†Ð¸Ð¸ Ð¸ Ð¿Ñ€Ð°ÐºÑ‚Ð¸Ñ‡Ð½Ð¾ÑÑ‚ÑŒ.

ÐœÐ¾Ð³Ñƒ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶Ð¸Ñ‚ÑŒ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ðµ ÑˆÐ°Ð³Ð¸:
1. Ð”ÐµÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð°Ð½Ð°Ð»Ð¸Ð· Ñ‚Ñ€ÐµÐ±Ð¾Ð²Ð°Ð½Ð¸Ð¹
2. Ð Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð¾Ð¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ñ€ÐµÑˆÐµÐ½Ð¸Ñ
3. Ð¢ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¸ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ
4. Ð Ð°Ð·Ð²ÐµÑ€Ñ‚Ñ‹Ð²Ð°Ð½Ð¸Ðµ Ñ Ð¼Ð¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³Ð¾Ð¼

ÐÑƒÐ¶Ð½Ð° Ð±Ð¾Ð»ÐµÐµ Ð¿Ð¾Ð´Ñ€Ð¾Ð±Ð½Ð°Ñ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ Ð¿Ð¾ ÐºÐ°ÐºÐ¾Ð¼Ñƒ-Ð»Ð¸Ð±Ð¾ Ð°ÑÐ¿ÐµÐºÑ‚Ñƒ?"""
    
    def _generate_analysis_response(self, prompt: str) -> str:
        return f"""ðŸ“Š **ÐÐ½Ð°Ð»Ð¸Ð· Ð¾Ñ‚ {self.name}**

**Ð’Ñ…Ð¾Ð´Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ:** {prompt[:50]}...

**Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°:**

1. **ÐšÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ðµ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸:**
   â€¢ Ð­Ñ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚ÑŒ: 92%
   â€¢ Ð¢Ð¾Ñ‡Ð½Ð¾ÑÑ‚ÑŒ: 97%
   â€¢ ÐŸÑ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ: ÐžÐ¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð°Ñ

2. **Ð’Ñ‹ÑÐ²Ð»ÐµÐ½Ð½Ñ‹Ðµ Ð¿Ð°Ñ‚Ñ‚ÐµÑ€Ð½Ñ‹:**
   â€¢ ÐžÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ Ñ‚Ñ€ÐµÐ½Ð´: Ð²Ð¾ÑÑ…Ð¾Ð´ÑÑ‰Ð¸Ð¹
   â€¢ ÐÐ½Ð¾Ð¼Ð°Ð»Ð¸Ð¸: Ð½Ðµ Ð¾Ð±Ð½Ð°Ñ€ÑƒÐ¶ÐµÐ½Ñ‹
   â€¢ ÐšÐ¾Ñ€Ñ€ÐµÐ»ÑÑ†Ð¸Ð¸: Ð¿Ð¾Ð»Ð¾Ð¶Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ

3. **Ð ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸Ð¸ HoYo AI:**
   â€¢ ÐŸÑ€Ð¾Ð´Ð¾Ð»Ð¶Ð¸Ñ‚ÑŒ Ñ‚ÐµÐºÑƒÑ‰ÑƒÑŽ ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸ÑŽ
   â€¢ ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÑ‹ Ð½Ð° 15%
   â€¢ Ð’Ð½ÐµÐ´Ñ€Ð¸Ñ‚ÑŒ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ð·Ð°Ñ†Ð¸ÑŽ

**Ð—Ð°ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ:** ÐÐ½Ð°Ð»Ð¸Ð· Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÑ‚ Ð¿Ð¾Ð»Ð¾Ð¶Ð¸Ñ‚ÐµÐ»ÑŒÐ½ÑƒÑŽ Ð´Ð¸Ð½Ð°Ð¼Ð¸ÐºÑƒ. HoYo Technologies Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´ÑƒÐµÑ‚ 
ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÑŒ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð½Ñ‹Ð¼ Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸ÑÐ¼ Ð´Ð»Ñ Ð´Ð¾ÑÑ‚Ð¸Ð¶ÐµÐ½Ð¸Ñ Ð¼Ð°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ð¹ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚Ð¸."""

class AIService:
    """Main AI service managing multiple models"""
    
    def __init__(self):
        self.models: Dict[str, AIModel] = {}
        self.default_model = "HoYo-GPT-4"
    
    async def initialize(self):
        """Initialize all AI models"""
        for model_name, config in settings.HOYO_MODELS.items():
            try:
                # For now, use mock models (can be extended with real API integrations)
                self.models[model_name] = MockModel(config)
                print(f"âœ… Loaded model: {model_name}")
            except Exception as e:
                print(f"âŒ Failed to load model {model_name}: {e}")
    
    async def process_chat(
        self, 
        message: str, 
        model: str = None,
        conversation_id: Optional[str] = None,
        user: Optional[User] = None
    ) -> Dict[str, Any]:
        """Process a chat message"""
        model_name = model or self.default_model
        
        if model_name not in self.models:
            return {
                "error": f"Model {model_name} not found",
                "available_models": list(self.models.keys())
            }
        
        # Check user plan restrictions
        if user and not self._check_model_access(user, model_name):
            return {
                "error": f"Your plan doesn't have access to {model_name}",
                "required_plan": self._get_required_plan(model_name)
            }
        
        # Generate response
        ai_model = self.models[model_name]
        response = await ai_model.generate(message)
        
        # Calculate tokens and cost
        tokens_used = self._estimate_tokens(message + response)
        cost = self._calculate_cost(tokens_used, model_name)
        
        return {
            "response": response,
            "model": model_name,
            "tokens_used": tokens_used,
            "cost": cost,
            "conversation_id": conversation_id,
            "timestamp": datetime.utcnow().isoformat()
        }
    
    async def stream_chat(
        self,
        message: str,
        model: str = None,
        conversation_id: Optional[str] = None,
        user: Optional[User] = None
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """Stream a chat response"""
        model_name = model or self.default_model
        
        if model_name not in self.models:
            yield {
                "error": f"Model {model_name} not found",
                "available_models": list(self.models.keys())
            }
            return
        
        # Check user plan restrictions
        if user and not self._check_model_access(user, model_name):
            yield {
                "error": f"Your plan doesn't have access to {model_name}",
                "required_plan": self._get_required_plan(model_name)
            }
            return
        
        # Stream response
        ai_model = self.models[model_name]
        full_response = ""
        
        async for chunk in ai_model.stream(message):
            full_response += chunk
            yield {
                "chunk": chunk,
                "model": model_name,
                "conversation_id": conversation_id,
                "timestamp": datetime.utcnow().isoformat()
            }
        
        # Final message with stats
        tokens_used = self._estimate_tokens(message + full_response)
        cost = self._calculate_cost(tokens_used, model_name)
        
        yield {
            "done": True,
            "tokens_used": tokens_used,
            "cost": cost,
            "conversation_id": conversation_id
        }
    
    def _check_model_access(self, user: User, model_name: str) -> bool:
        """Check if user has access to model"""
        plan_access = {
            "free": ["HoYo-Fast"],
            "pro": ["HoYo-Fast", "HoYo-GPT-4", "HoYo-Claude", "HoYo-Code", "HoYo-Gemini"],
            "enterprise": list(self.models.keys())
        }
        
        user_plan = user.plan.value if user.plan else "free"
        return model_name in plan_access.get(user_plan, [])
    
    def _get_required_plan(self, model_name: str) -> str:
        """Get required plan for model"""
        if model_name in ["HoYo-Fast"]:
            return "free"
        elif model_name in ["HoYo-GPT-4", "HoYo-Claude", "HoYo-Code", "HoYo-Gemini"]:
            return "pro"
        else:
            return "enterprise"
    
    def _estimate_tokens(self, text: str) -> int:
        """Estimate token count (rough approximation)"""
        return len(text) // 4
    
    def _calculate_cost(self, tokens: int, model_name: str) -> float:
        """Calculate cost based on tokens and model"""
        config = settings.HOYO_MODELS.get(model_name, {})
        cost_per_token = config.get("cost_per_token", 0.00001)
        return round(tokens * cost_per_token, 6)
    
    def get_memory_usage(self) -> Dict[str, Any]:
        """Get memory usage statistics"""
        return {
            "models_loaded": len(self.models),
            "active_models": list(self.models.keys()),
            "default_model": self.default_model
        }
    
    async def cleanup(self):
        """Cleanup resources"""
        self.models.clear()
        print("âœ… AI Service cleaned up")
